---
services:
  vmalert:
    image: victoriametrics/vmalert:v1.128.0
    depends_on:
      - victoriametrics
      - alertmanager
    configs:
      - source: vmalert_alerts_yml
        target: /etc/alerts.yml
      - source: vmalert_alerts_health_yml
        target: /etc/alerts-health.yml
      - source: vmalert_alerts_vmagent_yml
        target: /etc/alerts-vmagent.yml
      - source: vmalert_alerts_vmalert_yml
        target: /etc/alerts-vmalert.yml
      - source: vmalert_alerts_node_exporter_yml
        target: /etc/alerts-node-exporter.yml
      - source: vmalert_alerts_cadvisor_yml
        target: /etc/alerts-cadvisor.yml
      - source: vmalert_alerts_caddy_yml
        target: /etc/alerts-caddy.yml
      - source: vmalert_alerts_blackbox_yml
        target: /etc/alerts-blackbox.yml
    command:
      - "--datasource.url=http://victoriametrics:8428/"
      - "--remoteRead.url=http://victoriametrics:8428/"
      - "--remoteWrite.url=http://vmagent:8429/"
      - "--notifier.url=http://alertmanager:9093/"
      - "--rule=/etc/alerts-*.yml"
      - "--external.url=https://grafana.${TAILNET}"
      - '--external.alert.source=explore?orgId=1&left={"datasource":"VictoriaMetrics","queries":[{"expr":{{.Expr|jsonEscape|queryEscape}},"refId":"A"}],"range":{"from":"{{ .ActiveAt.UnixMilli }}","to":"now"}}'
    restart: always

configs:
  vmalert_alerts_yml:
    content: |
      ---
      # File contains default list of alerts for VictoriaMetrics single server.
      # The alerts below are just recommendations and may require some updates
      # and threshold calibration according to every specific setup.
      groups:
        # Alerts group for VM single assumes that Grafana dashboard
        # https://grafana.com/grafana/dashboards/10229 is installed.
        # Pls update the `dashboard` annotation according to your setup.
        - name: vmsingle
          interval: 30s
          concurrency: 2
          rules:
            - alert: DiskRunsOutOfSpaceIn3Days
              expr: |
                  sum(vm_free_disk_space_bytes) without(path) /
                  (
                    (rate(vm_rows_added_to_storage_total[1d]) - sum(rate(vm_deduplicated_samples_total[1d])) without(type)) * (
                      sum(vm_data_size_bytes{type!~"indexdb.*"}) without(type) /
                      sum(vm_rows{type!~"indexdb.*"}) without(type)
                    )
                    +
                    rate(vm_new_timeseries_created_total[1d]) * (
                      sum(vm_data_size_bytes{type="indexdb/file"}) without(type)/
                      sum(vm_rows{type="indexdb/file"}) without(type)
                    )
                  ) < 3 * 24 * 3600 > 0
              for: 30m
              labels:
                severity: critical
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/wNf0q_kZk?viewPanel=53&var-instance={{ $$labels.instance }}"
                summary: "Instance {{ $$labels.instance }} will run out of disk space soon"
                description: "Taking into account current ingestion rate, free disk space will be enough only
                  for {{ $$value | humanizeDuration }} on instance {{ $$labels.instance }}.\n
                  Consider to limit the ingestion rate, decrease retention or scale the disk space if possible."
      
            - alert: NodeBecomesReadonlyIn3Days
              expr: |
                sum(vm_free_disk_space_bytes - vm_free_disk_space_limit_bytes) without(path) /
                (
                    (rate(vm_rows_added_to_storage_total[1d]) - sum(rate(vm_deduplicated_samples_total[1d])) without(type)) * (
                      sum(vm_data_size_bytes{type!~"indexdb.*"}) without(type) /
                      sum(vm_rows{type!~"indexdb.*"}) without(type)
                    )
                    +
                    rate(vm_new_timeseries_created_total[1d]) * (
                      sum(vm_data_size_bytes{type="indexdb/file"}) without(type) /
                      sum(vm_rows{type="indexdb/file"}) without(type)
                    )
                ) < 3 * 24 * 3600 > 0
              for: 30m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/oS7Bi_0Wz?viewPanel=53&var-instance={{ $$labels.instance }}"
                summary: "Instance {{ $$labels.instance }} will become read-only in 3 days"
                description: "Taking into account current ingestion rate and free disk space
                    instance {{ $$labels.instance }} is writable for {{ $$value | humanizeDuration }}.\n
                    Consider to limit the ingestion rate, decrease retention or scale the disk space up if possible."
      
            - alert: DiskRunsOutOfSpace
              expr: |
                sum(vm_data_size_bytes) by(job, instance) /
                (
                 sum(vm_free_disk_space_bytes) by(job, instance) +
                 sum(vm_data_size_bytes) by(job, instance)
                ) > 0.8
              for: 30m
              labels:
                severity: critical
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/wNf0q_kZk?viewPanel=53&var-instance={{ $$labels.instance }}"
                summary: "Instance {{ $$labels.instance }} (job={{ $$labels.job }}) will run out of disk space soon"
                description: "Disk utilisation on instance {{ $$labels.instance }} is more than 80%.\n
                  Having less than 20% of free disk space could cripple merge processes and overall performance.
                  Consider to limit the ingestion rate, decrease retention or scale the disk space if possible."
      
            - alert: RequestErrorsToAPI
              expr: increase(vm_http_request_errors_total[5m]) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/wNf0q_kZk?viewPanel=35&var-instance={{ $$labels.instance }}"
                summary: "Too many errors served for path {{ $$labels.path }} (instance {{ $$labels.instance }})"
                description: "Requests to path {{ $$labels.path }} are receiving errors.
                  Please verify if clients are sending correct requests."
      
            - alert: TooHighChurnRate
              expr: |
                (
                   sum(rate(vm_new_timeseries_created_total[5m])) by(instance)
                   /
                   sum(rate(vm_rows_inserted_total[5m])) by(instance)
                 ) > 0.1
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/wNf0q_kZk?viewPanel=66&var-instance={{ $$labels.instance }}"
                summary: "Churn rate is more than 10% on \"{{ $$labels.instance }}\" for the last 15m"
                description: "VM constantly creates new time series on \"{{ $$labels.instance }}\".\n
                  This effect is known as Churn Rate.\n
                  High Churn Rate is tightly connected with database performance and may
                  result in unexpected OOM's or slow queries."
      
            - alert: TooHighChurnRate24h
              expr: |
                sum(increase(vm_new_timeseries_created_total[24h])) by(instance)
                >
                (sum(vm_cache_entries{type="storage/hour_metric_ids"}) by(instance) * 3)
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/wNf0q_kZk?viewPanel=66&var-instance={{ $$labels.instance }}"
                summary: "Too high number of new series on \"{{ $$labels.instance }}\" created over last 24h"
                description: "The number of created new time series over last 24h is 3x times higher than
                  current number of active series on \"{{ $$labels.instance }}\".\n
                  This effect is known as Churn Rate.\n
                  High Churn Rate is tightly connected with database performance and may
                  result in unexpected OOM's or slow queries."
      
            - alert: TooHighSlowInsertsRate
              expr: |
                (
                   sum(rate(vm_slow_row_inserts_total[5m])) by(instance)
                   /
                   sum(rate(vm_rows_inserted_total[5m])) by(instance)
                 ) > 0.05
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/wNf0q_kZk?viewPanel=68&var-instance={{ $$labels.instance }}"
                summary: "Percentage of slow inserts is more than 5% on \"{{ $$labels.instance }}\" for the last 15m"
                description: "High rate of slow inserts on \"{{ $$labels.instance }}\" may be a sign of resource exhaustion
                  for the current load. It is likely more RAM is needed for optimal handling of the current number of active time series.
                  See also https://github.com/VictoriaMetrics/VictoriaMetrics/issues/3976#issuecomment-1476883183"

  
  vmalert_alerts_health_yml:
    content: |
      ---
      # File contains default list of alerts for various VM components.
      # The following alerts are recommended for use for any VM installation.
      # The alerts below are just recommendations and may require some updates
      # and threshold calibration according to every specific setup.
      groups:
        - name: vm-health
          # note the `job` filter and update accordingly to your setup
          rules:
            - alert: TooManyRestarts
              expr: changes(process_start_time_seconds{job=~".*(victoriametrics|vmselect|vminsert|vmstorage|vmagent|vmalert|vmsingle|vmalertmanager|vmauth).*"}[15m]) > 2
              labels:
                severity: critical
              annotations:
                summary: "{{ $$labels.job }} too many restarts (instance {{ $$labels.instance }})"
                description: >
                  Job {{ $$labels.job }} (instance {{ $$labels.instance }}) has restarted more than twice in the last 15 minutes.
                  It might be crashlooping.
      
            - alert: ServiceDown
              expr: up{job=~".*(victoriametrics|vmselect|vminsert|vmstorage|vmagent|vmalert|vmsingle|vmalertmanager|vmauth).*"} == 0
              for: 2m
              labels:
                severity: critical
              annotations:
                summary: "Service {{ $$labels.job }} is down on {{ $$labels.instance }}"
                description: "{{ $$labels.instance }} of job {{ $$labels.job }} has been down for more than 2 minutes."
      
            - alert: ProcessNearFDLimits
              expr: (process_max_fds - process_open_fds) < 100
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "Number of free file descriptors is less than 100 for \"{{ $$labels.job }}\"(\"{{ $$labels.instance }}\") for the last 5m"
                description: | 
                  Exhausting OS file descriptors limit can cause severe degradation of the process.
                  Consider to increase the limit as fast as possible.
      
            - alert: TooHighMemoryUsage
              expr: (min_over_time(process_resident_memory_anon_bytes[10m]) / vm_available_memory_bytes) > 0.8
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "It is more than 80% of memory used by \"{{ $$labels.job }}\"(\"{{ $$labels.instance }}\")"
                description: |
                  Too high memory usage may result into multiple issues such as OOMs or degraded performance.
                  Consider to either increase available memory or decrease the load on the process.
      
            - alert: TooHighCPUUsage
              expr: rate(process_cpu_seconds_total[5m]) / process_cpu_cores_available > 0.9
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "More than 90% of CPU is used by \"{{ $$labels.job }}\"(\"{{ $$labels.instance }}\") during the last 5m"
                description: >
                  Too high CPU usage may be a sign of insufficient resources and make process unstable.
                  Consider to either increase available CPU resources or decrease the load on the process.
      
            - alert: TooHighGoroutineSchedulingLatency
              expr: histogram_quantile(0.99, sum(rate(go_sched_latencies_seconds_bucket{job=~".*(victoriametrics|vmselect|vminsert|vmstorage|vmagent|vmalert|vmsingle|vmalertmanager|vmauth).*"}[5m])) by (le, job, instance)) > 0.1
              for: 15m
              labels:
                severity: critical
              annotations:
                summary: "\"{{ $$labels.job }}\"(\"{{ $$labels.instance }}\") has insufficient CPU resources for >15m"
                description: >
                  Go runtime is unable to schedule goroutines execution in acceptable time. This is usually a sign of
                  insufficient CPU resources or CPU throttling. Verify that service has enough CPU resources. Otherwise,
                  the service could work unreliably with delays in processing.
      
            - alert: TooManyLogs
              expr: sum(increase(vm_log_messages_total{level="error"}[5m])) without (app_version, location) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: "Too many logs printed for job \"{{ $$labels.job }}\" ({{ $$labels.instance }})"
                description: >
                  Logging rate for job \"{{ $$labels.job }}\" ({{ $$labels.instance }}) is {{ $$value }} for last 15m.
                  Worth to check logs for specific error messages.
      
            - alert: TooManyTSIDMisses
              expr: rate(vm_missing_tsids_for_metric_id_total[5m]) > 0
              for: 10m
              labels:
                severity: critical
              annotations:
                summary: "Too many TSID misses for job \"{{ $$labels.job }}\" ({{ $$labels.instance }})"
                description: | 
                  The rate of TSID misses during query lookups is too high for \"{{ $$labels.job }}\" ({{ $$labels.instance }}).
                  Make sure you're running VictoriaMetrics of v1.85.3 or higher.
                  Related issue https://github.com/VictoriaMetrics/VictoriaMetrics/issues/3502
      
            - alert: ConcurrentInsertsHitTheLimit
              expr: avg_over_time(vm_concurrent_insert_current[1m]) >= vm_concurrent_insert_capacity
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: "{{ $$labels.job }} on instance {{ $$labels.instance }} is constantly hitting concurrent inserts limit"
                description: | 
                  The limit of concurrent inserts on instance {{ $$labels.instance }} depends on the number of CPUs.
                  Usually, when component constantly hits the limit it is likely the component is overloaded and requires more CPU.
                  In some cases for components like vmagent or vminsert the alert might trigger if there are too many clients
                  making write attempts. If vmagent's or vminsert's CPU usage and network saturation are at normal level, then 
                  it might be worth adjusting `-maxConcurrentInserts` cmd-line flag.
      
            - alert: IndexDBRecordsDrop
              expr: increase(vm_indexdb_items_dropped_total[5m]) > 0
              labels:
                severity: critical
              annotations:
                summary: "IndexDB skipped registering items during data ingestion with reason={{ $$labels.reason }}."
                description: | 
                  VictoriaMetrics could skip registering new timeseries during ingestion if they fail the validation process. 
                  For example, `reason=too_long_item` means that time series cannot exceed 64KB. Please, reduce the number 
                  of labels or label values for such series. Or enforce these limits via `-maxLabelsPerTimeseries` and 
                  `-maxLabelValueLen` command-line flags.
      
            - alert: RowsRejectedOnIngestion
              expr: rate(vm_rows_ignored_total[5m]) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: "Some rows are rejected on \"{{ $$labels.instance }}\" on ingestion attempt"
                description: "Ingested rows on instance \"{{ $$labels.instance }}\" are rejected due to the
                  following reason: \"{{ $$labels.reason }}\""
      
            - alert: TooHighQueryLoad
              expr: increase(vm_concurrent_select_limit_timeout_total[5m]) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: "Read queries fail with timeout for {{ $$labels.job }} on instance {{ $$labels.instance }}"
                description: |
                  Instance {{ $$labels.instance }} ({{ $$labels.job }}) is failing to serve read queries during last 15m.
                  Concurrency limit `-search.maxConcurrentRequests` was reached on this instance and extra queries were
                  put into the queue for `-search.maxQueueDuration` interval. But even after waiting in the queue these queries weren't served.
                  This happens if instance is overloaded with the current workload, or datasource is too slow to respond.
                  Possible solutions are the following:
                  * reduce the query load;
                  * increase compute resources or number of replicas;
                  * adjust limits `-search.maxConcurrentRequests` and `-search.maxQueueDuration`.
                  See more at https://docs.victoriametrics.com/victoriametrics/troubleshooting/#slow-queries.
      
  vmalert_alerts_vmagent_yml:
    content: |
      ---
      # File contains default list of alerts for vmagent service.
      # The alerts below are just recommendations and may require some updates
      # and threshold calibration according to every specific setup.
      groups:
        # Alerts group for vmagent assumes that Grafana dashboard
        # https://grafana.com/grafana/dashboards/12683 is installed.
        # Pls update the `dashboard` annotation according to your setup.
        - name: vmagent
          interval: 30s
          concurrency: 2
          rules:
            - alert: PersistentQueueIsDroppingData
              expr: sum(increase(vm_persistentqueue_bytes_dropped_total[5m])) without (path) > 0
              for: 10m
              labels:
                severity: critical
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/G7Z9GzMGz?viewPanel=49&var-instance={{ $$labels.instance }}"
                summary: "Instance {{ $$labels.instance }} is dropping data from persistent queue"
                description: "Vmagent dropped {{ $$value | humanize1024 }} from persistent queue
                    on instance {{ $$labels.instance }} for the last 10m."
      
            - alert: RejectedRemoteWriteDataBlocksAreDropped
              expr: sum(increase(vmagent_remotewrite_packets_dropped_total[5m])) without (url) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/G7Z9GzMGz?viewPanel=79&var-instance={{ $$labels.instance }}"
                summary: "Vmagent is dropping data blocks that are rejected by remote storage"
                description: "Job \"{{ $$labels.job }}\" on instance {{ $$labels.instance }} drops the rejected by 
                  remote-write server data blocks. Check the logs to find the reason for rejects."
      
            - alert: TooManyScrapeErrors
              expr: increase(vm_promscrape_scrapes_failed_total[5m]) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/G7Z9GzMGz?viewPanel=31&var-instance={{ $$labels.instance }}"
                summary: "Vmagent fails to scrape one or more targets"
                description: "Job \"{{ $$labels.job }}\" on instance {{ $$labels.instance }} fails to scrape targets for last 15m"
      
            - alert: ScrapePoolHasNoTargets
              expr: sum(vm_promscrape_scrape_pool_targets) without (status, instance, pod) == 0
              for: 30m
              labels:
                severity: warning
              annotations:
                summary: "Vmagent has scrape_pool with 0 configured/discovered targets"
                description: "Vmagent \"{{ $$labels.job }}\" has scrape_pool \"{{ $$labels.scrape_job }}\"
                  with 0 discovered targets. It is likely a misconfiguration. Please follow https://docs.victoriametrics.com/victoriametrics/vmagent/#debugging-scrape-targets
                  to troubleshoot the scraping config."
      
            - alert: TooManyWriteErrors
              expr: |
                (sum(increase(vm_ingestserver_request_errors_total[5m])) without (name,net,type)
                +
                sum(increase(vmagent_http_request_errors_total[5m])) without (path,protocol)) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/G7Z9GzMGz?viewPanel=77&var-instance={{ $$labels.instance }}"
                summary: "Vmagent responds with too many errors on data ingestion protocols"
                description: "Job \"{{ $$labels.job }}\" on instance {{ $$labels.instance }} responds with errors to write requests for last 15m."
      
            - alert: TooManyRemoteWriteErrors
              expr: rate(vmagent_remotewrite_retries_count_total[5m]) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/G7Z9GzMGz?viewPanel=61&var-instance={{ $$labels.instance }}"
                summary: "Job \"{{ $$labels.job }}\" on instance {{ $$labels.instance }} fails to push to remote storage"
                description: "Vmagent fails to push data via remote write protocol to destination \"{{ $$labels.url }}\"\n
                  Ensure that destination is up and reachable."
      
            - alert: RemoteWriteConnectionIsSaturated
              expr: |
                (
                 rate(vmagent_remotewrite_send_duration_seconds_total[5m])
                 / 
                 vmagent_remotewrite_queues
                ) > 0.9
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/G7Z9GzMGz?viewPanel=84&var-instance={{ $$labels.instance }}"
                summary: "Remote write connection from \"{{ $$labels.job }}\" (instance {{ $$labels.instance }}) to {{ $$labels.url }} is saturated"
                description: "The remote write connection between vmagent \"{{ $$labels.job }}\" (instance {{ $$labels.instance }}) and destination \"{{ $$labels.url }}\"
                  is saturated by more than 90% and vmagent won't be able to keep up.\n
                  There could be the following reasons for this:\n
                   * vmagent can't send data fast enough through the existing network connections. Increase `-remoteWrite.queues` cmd-line flag value to establish more connections per destination.\n
                   * remote destination can't accept data fast enough. Check if remote destination has enough resources for processing."
      
            - alert: PersistentQueueForWritesIsSaturated
              expr: rate(vm_persistentqueue_write_duration_seconds_total[5m]) > 0.9
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/G7Z9GzMGz?viewPanel=98&var-instance={{ $$labels.instance }}"
                summary: "Persistent queue writes for instance {{ $$labels.instance }} are saturated"
                description: "Persistent queue writes for vmagent \"{{ $$labels.job }}\" (instance {{ $$labels.instance }})
                  are saturated by more than 90% and vmagent won't be able to keep up with flushing data on disk. 
                  In this case, consider to decrease load on the vmagent or improve the disk throughput."
      
            - alert: PersistentQueueForReadsIsSaturated
              expr: rate(vm_persistentqueue_read_duration_seconds_total[5m]) > 0.9
              for: 15m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/G7Z9GzMGz?viewPanel=99&var-instance={{ $$labels.instance }}"
                summary: "Persistent queue reads for instance {{ $$labels.instance }} are saturated"
                description: "Persistent queue reads for vmagent \"{{ $$labels.job }}\" (instance {{ $$labels.instance }})
                  are saturated by more than 90% and vmagent won't be able to keep up with reading data from the disk. 
                  In this case, consider to decrease load on the vmagent or improve the disk throughput."
      
            - alert: SeriesLimitHourReached
              expr: (vmagent_hourly_series_limit_current_series / vmagent_hourly_series_limit_max_series) > 0.9
              labels:
                severity: critical
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/G7Z9GzMGz?viewPanel=88&var-instance={{ $$labels.instance }}"
                summary: "Instance {{ $$labels.instance }} reached 90% of the limit"
                description: "Max series limit set via -remoteWrite.maxHourlySeries flag is close to reaching the max value. 
                  Then samples for new time series will be dropped instead of sending them to remote storage systems."
      
            - alert: SeriesLimitDayReached
              expr: (vmagent_daily_series_limit_current_series / vmagent_daily_series_limit_max_series) > 0.9
              labels:
                severity: critical
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/G7Z9GzMGz?viewPanel=90&var-instance={{ $$labels.instance }}"
                summary: "Instance {{ $$labels.instance }} reached 90% of the limit"
                description: "Max series limit set via -remoteWrite.maxDailySeries flag is close to reaching the max value. 
                  Then samples for new time series will be dropped instead of sending them to remote storage systems."
      
            - alert: ConfigurationReloadFailure
              expr: |
                vm_promscrape_config_last_reload_successful != 1
                or
                vmagent_relabel_config_last_reload_successful != 1
              labels:
                severity: warning
              annotations:
                summary: "Configuration reload failed for vmagent instance {{ $$labels.instance }}"
                description: "Configuration hot-reload failed for vmagent on instance {{ $$labels.instance }}.
                Check vmagent's logs for detailed error message."
      
            - alert: StreamAggrFlushTimeout
              expr: |
                increase(vm_streamaggr_flush_timeouts_total[5m]) > 0
              labels:
                severity: warning
              annotations:
                summary: "Streaming aggregation at \"{{ $$labels.job }}\" (instance {{ $$labels.instance }}) can't be finished within the configured aggregation interval."
                description: "Stream aggregation process can't keep up with the load and might produce incorrect aggregation results. Check logs for more details.
                  Possible solutions: increase aggregation interval; aggregate smaller number of series; reduce samples' ingestion rate to stream aggregation."
      
            - alert: StreamAggrDedupFlushTimeout
              expr: |
                increase(vm_streamaggr_dedup_flush_timeouts_total[5m]) > 0
              labels:
                severity: warning
              annotations:
                summary: "Deduplication \"{{ $$labels.job }}\" (instance {{ $$labels.instance }}) can't be finished within configured deduplication interval."
                description: "Deduplication process can't keep up with the load and might produce incorrect results. Check docs https://docs.victoriametrics.com/victoriametrics/stream-aggregation/#deduplication and logs for more details.
                  Possible solutions: increase deduplication interval; deduplicate smaller number of series; reduce samples' ingestion rate."
      
  vmalert_alerts_vmalert_yml:
    content: |
      ---
      # File contains default list of alerts for vmalert service.
      # The alerts below are just recommendations and may require some updates
      # and threshold calibration according to every specific setup.
      groups:
        # Alerts group for vmalert assumes that Grafana dashboard
        # https://grafana.com/grafana/dashboards/14950 is installed.
        # Pls update the `dashboard` annotation according to your setup.
        - name: vmalert
          interval: 30s
          rules:
            - alert: ConfigurationReloadFailure
              expr: vmalert_config_last_reload_successful != 1
              labels:
                severity: warning
              annotations:
                summary: "Configuration reload failed for vmalert instance {{ $$labels.instance }}"
                description: "Configuration hot-reload failed for vmalert on instance {{ $$labels.instance }}.
                  Check vmalert's logs for detailed error message."
      
            - alert: AlertingRulesError
              expr: sum(increase(vmalert_alerting_rules_errors_total[5m])) without(id) > 0
              for: 5m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/LzldHAVnz?viewPanel=13&var-instance={{ $$labels.instance }}&var-file={{ $$labels.file }}&var-group={{ $$labels.group }}"
                summary: "Alerting rules are failing for vmalert instance {{ $$labels.instance }}"
                description: "Alerting rules execution is failing for \"{{ $$labels.alertname }}\" from group \"{{ $$labels.group }}\" in file \"{{ $$labels.file }}\".
                  Check vmalert's logs for detailed error message."
      
            - alert: RecordingRulesError
              expr: sum(increase(vmalert_recording_rules_errors_total[5m])) without(id) > 0
              for: 5m
              labels:
                severity: warning
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/LzldHAVnz?viewPanel=30&var-instance={{ $$labels.instance }}&var-file={{ $$labels.file }}&var-group={{ $$labels.group }}"
                summary: "Recording rules are failing for vmalert instance {{ $$labels.instance }}"
                description: "Recording rules execution is failing for \"{{ $$labels.recording }}\" from group \"{{ $$labels.group }}\" in file \"{{ $$labels.file }}\".
                  Check vmalert's logs for detailed error message."
      
            - alert: RecordingRulesNoData
              expr: sum(vmalert_recording_rules_last_evaluation_samples) without(id) < 1
              for: 30m
              labels:
                severity: info
              annotations:
                dashboard: "https://grafana.${TAILNET}/d/LzldHAVnz?viewPanel=33&var-file={{ $$labels.file }}&var-group={{ $$labels.group }}"
                summary: "Recording rule {{ $$labels.recording }} ({{ $$labels.group }}) produces no data"
                description: "Recording rule \"{{ $$labels.recording }}\" from group \"{{ $$labels.group }}\ in file \"{{ $$labels.file }}\" 
                  produces 0 samples over the last 30min. It might be caused by a misconfiguration 
                  or incorrect query expression."
      
            - alert: TooManyMissedIterations
              expr: increase(vmalert_iteration_missed_total[5m]) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: "vmalert instance {{ $$labels.instance }} is missing rules evaluations"
                description: "vmalert instance {{ $$labels.instance }} is missing rules evaluations for group \"{{ $$labels.group }}\" in file \"{{ $$labels.file }}\".
                  The group evaluation time takes longer than the configured evaluation interval. This may result in missed 
                  alerting notifications or recording rules samples. Try increasing evaluation interval or concurrency of
                  group \"{{ $$labels.group }}\". See https://docs.victoriametrics.com/victoriametrics/vmalert/#groups. 
                  If rule expressions are taking longer than expected, please see https://docs.victoriametrics.com/victoriametrics/troubleshooting/#slow-queries."
      
            - alert: RemoteWriteErrors
              expr: increase(vmalert_remotewrite_errors_total[5m]) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: "vmalert instance {{ $$labels.instance }} is failing to push metrics to remote write URL"
                description: "vmalert instance {{ $$labels.instance }} is failing to push metrics generated via alerting 
                  or recording rules to the configured remote write URL. Check vmalert's logs for detailed error message."
      
            - alert: RemoteWriteDroppingData
              expr: increase(vmalert_remotewrite_dropped_rows_total[5m]) > 0
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "vmalert instance {{ $$labels.instance }} is dropping data sent to remote write URL"
                description: "vmalert instance {{ $$labels.instance }} is failing to send results of alerting or recording rules 
                  to the configured remote write URL. This may result into gaps in recording rules or alerts state.
                  Check vmalert's logs for detailed error message."
      
            - alert: AlertmanagerErrors
              expr: increase(vmalert_alerts_send_errors_total[5m]) > 0
              for: 15m
              labels:
                severity: warning
              annotations:
                summary: "vmalert instance {{ $$labels.instance }} is failing to send notifications to Alertmanager"
                description: "vmalert instance {{ $$labels.instance }} is failing to send alert notifications to \"{{ $$labels.addr }}\".
                  Check vmalert's logs for detailed error message."

            - alert: VictoriaMetricsAlertmanagerE2eDeadManSwitch
              expr: vector(1)
              for: 0m
              labels:
                severity: none
                monitor: self
              annotations:
                summary: VictoriaMetrics AlertManager E2E dead man switch (instance {{ $$labels.instance }})
                description: "VictoriaMetrics DeadManSwitch is an always-firing alert. It's used as an end-to-end test of VictoriaMetrics through the Alertmanager.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

  vmalert_alerts_node_exporter_yml:
    content: |
      ---
      groups:
      - name: NodeExporter
        rules:
          - alert: HostOutOfMemory
            expr: '(node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes < .10)'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host out of memory (instance {{ $$labels.instance }})
              description: "Node memory is filling up (< 10% left)\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostMemoryUnderMemoryPressure
            expr: '(rate(node_vmstat_pgmajfault[5m]) > 1000)'
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host memory under memory pressure (instance {{ $$labels.instance }})
              description: "The node is under heavy memory pressure. High rate of loading memory pages from disk.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostMemoryIsUnderutilized
            expr: 'min_over_time(node_memory_MemFree_bytes[1w]) > node_memory_MemTotal_bytes * .8'
            for: 0m
            labels:
              severity: info
            annotations:
              summary: Host Memory is underutilized (instance {{ $$labels.instance }})
              description: "Node memory usage is < 20% for 1 week. Consider reducing memory space. (instance {{ $$labels.instance }})\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostUnusualNetworkThroughputIn
            expr: '((rate(node_network_receive_bytes_total[5m]) / on(instance, device) node_network_speed_bytes) > .80)'
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host unusual network throughput in (instance {{ $$labels.instance }})
              description: "Host receive bandwidth is high (>80%).\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostUnusualNetworkThroughputOut
            expr: '((rate(node_network_transmit_bytes_total[5m]) / on(instance, device) node_network_speed_bytes) > .80)'
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host unusual network throughput out (instance {{ $$labels.instance }})
              description: "Host transmit bandwidth is high (>80%)\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostUnusualDiskReadRate
            expr: '(rate(node_disk_io_time_seconds_total[5m]) > .80)'
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host unusual disk read rate (instance {{ $$labels.instance }})
              description: "Disk is too busy (IO wait > 80%)\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostOutOfDiskSpace
            expr: '(node_filesystem_avail_bytes{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"} / node_filesystem_size_bytes < .10 and on (instance, device, mountpoint) node_filesystem_readonly == 0)'
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host out of disk space (instance {{ $$labels.instance }})
              description: "Disk is almost full (< 10% left)\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostDiskMayFillIn24Hours
            expr: 'predict_linear(node_filesystem_avail_bytes{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"}[3h], 86400) <= 0 and node_filesystem_avail_bytes > 0'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host disk may fill in 24 hours (instance {{ $$labels.instance }})
              description: "Filesystem will likely run out of space within the next 24 hours.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostOutOfInodes
            expr: '(node_filesystem_files_free / node_filesystem_files < .10 and ON (instance, device, mountpoint) node_filesystem_readonly == 0)'
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host out of inodes (instance {{ $$labels.instance }})
              description: "Disk is almost running out of available inodes (< 10% left)\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostFilesystemDeviceError
            expr: 'node_filesystem_device_error{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"} == 1'
            for: 2m
            labels:
              severity: critical
            annotations:
              summary: Host filesystem device error (instance {{ $$labels.instance }})
              description: "Error stat-ing the {{ $$labels.mountpoint }} filesystem\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostInodesMayFillIn24Hours
            expr: 'predict_linear(node_filesystem_files_free{fstype!~"^(fuse.*|tmpfs|cifs|nfs)"}[1h], 86400) <= 0 and node_filesystem_files_free > 0'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host inodes may fill in 24 hours (instance {{ $$labels.instance }})
              description: "Filesystem will likely run out of inodes within the next 24 hours at current write rate\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostUnusualDiskReadLatency
            expr: '(rate(node_disk_read_time_seconds_total[1m]) / rate(node_disk_reads_completed_total[1m]) > 0.1 and rate(node_disk_reads_completed_total[1m]) > 0)'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host unusual disk read latency (instance {{ $$labels.instance }})
              description: "Disk latency is growing (read operations > 100ms)\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostUnusualDiskWriteLatency
            expr: '(rate(node_disk_write_time_seconds_total[1m]) / rate(node_disk_writes_completed_total[1m]) > 0.1 and rate(node_disk_writes_completed_total[1m]) > 0)'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host unusual disk write latency (instance {{ $$labels.instance }})
              description: "Disk latency is growing (write operations > 100ms)\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostHighCpuLoad
            expr: '1 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m]))) > .80'
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: Host high CPU load (instance {{ $$labels.instance }})
              description: "CPU load is > 80%\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostCpuIsUnderutilized
            expr: '(min by (instance) (rate(node_cpu_seconds_total{mode="idle"}[1h]))) > 0.8'
            for: 1w
            labels:
              severity: info
            annotations:
              summary: Host CPU is underutilized (instance {{ $$labels.instance }})
              description: "CPU load has been < 20% for 1 week. Consider reducing the number of CPUs.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostCpuStealNoisyNeighbor
            expr: 'avg by(instance) (rate(node_cpu_seconds_total{mode="steal"}[5m])) * 100 > 10'
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host CPU steal noisy neighbor (instance {{ $$labels.instance }})
              description: "CPU steal is > 10%. A noisy neighbor is killing VM performances or a spot instance may be out of credit.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostCpuHighIowait
            expr: 'avg by (instance) (rate(node_cpu_seconds_total{mode="iowait"}[5m])) > .10'
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host CPU high iowait (instance {{ $$labels.instance }})
              description: "CPU iowait > 10%. Your CPU is idling waiting for storage to respond.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostUnusualDiskIo
            expr: 'rate(node_disk_io_time_seconds_total[5m]) > 0.8'
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: Host unusual disk IO (instance {{ $$labels.instance }})
              description: "Disk usage >80%. Check storage for issues or increase IOPS capabilities. Check storage for issues.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostSwapIsFillingUp
            expr: '((1 - (node_memory_SwapFree_bytes / node_memory_SwapTotal_bytes)) * 100 > 80)'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host swap is filling up (instance {{ $$labels.instance }})
              description: "Swap is filling up (>80%)\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostSystemdServiceCrashed
            expr: '(node_systemd_unit_state{state="failed"} == 1)'
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host systemd service crashed (instance {{ $$labels.instance }})
              description: "systemd service crashed\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostPhysicalComponentTooHot
            expr: 'node_hwmon_temp_celsius > node_hwmon_temp_max_celsius'
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: Host physical component too hot (instance {{ $$labels.instance }})
              description: "Physical hardware component too hot\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostNodeOvertemperatureAlarm
            expr: '((node_hwmon_temp_crit_alarm_celsius == 1) or (node_hwmon_temp_alarm == 1))'
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Host node overtemperature alarm (instance {{ $$labels.instance }})
              description: "Physical node temperature alarm triggered\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostSoftwareRaidInsufficientDrives
            expr: '((node_md_disks_required - on(device, instance) node_md_disks{state="active"}) > 0)'
            for: 0m
            labels:
              severity: critical
            annotations:
              summary: Host software RAID insufficient drives (instance {{ $$labels.instance }})
              description: "MD RAID array {{ $$labels.device }} on {{ $$labels.instance }} has insufficient drives remaining.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostSoftwareRaidDiskFailure
            expr: '(node_md_disks{state="failed"} > 0)'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host software RAID disk failure (instance {{ $$labels.instance }})
              description: "MD RAID array {{ $$labels.device }} on {{ $$labels.instance }} needs attention.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostOomKillDetected
            expr: '(increase(node_vmstat_oom_kill[1m]) > 0)'
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host OOM kill detected (instance {{ $$labels.instance }})
              description: "OOM kill detected\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostEdacCorrectableErrorsDetected
            expr: '(increase(node_edac_correctable_errors_total[1m]) > 0)'
            for: 0m
            labels:
              severity: info
            annotations:
              summary: Host EDAC Correctable Errors detected (instance {{ $$labels.instance }})
              description: "Host {{ $$labels.instance }} has had {{ printf \"%.0f\" $$value }} correctable memory errors reported by EDAC in the last 5 minutes.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostEdacUncorrectableErrorsDetected
            expr: '(node_edac_uncorrectable_errors_total > 0)'
            for: 0m
            labels:
              severity: warning
            annotations:
              summary: Host EDAC Uncorrectable Errors detected (instance {{ $$labels.instance }})
              description: "Host {{ $$labels.instance }} has had {{ printf \"%.0f\" $$value }} uncorrectable memory errors reported by EDAC in the last 5 minutes.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostNetworkReceiveErrors
            expr: '(rate(node_network_receive_errs_total[2m]) / rate(node_network_receive_packets_total[2m]) > 0.01)'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host Network Receive Errors (instance {{ $$labels.instance }})
              description: "Host {{ $$labels.instance }} interface {{ $$labels.device }} has encountered {{ printf \"%.0f\" $$value }} receive errors in the last two minutes.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostNetworkTransmitErrors
            expr: '(rate(node_network_transmit_errs_total[2m]) / rate(node_network_transmit_packets_total[2m]) > 0.01)'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host Network Transmit Errors (instance {{ $$labels.instance }})
              description: "Host {{ $$labels.instance }} interface {{ $$labels.device }} has encountered {{ printf \"%.0f\" $$value }} transmit errors in the last two minutes.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostNetworkBondDegraded
            expr: '((node_bonding_active - node_bonding_slaves) != 0)'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host Network Bond Degraded (instance {{ $$labels.instance }})
              description: "Bond \"{{ $$labels.device }}\" degraded on \"{{ $$labels.instance }}\".\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostConntrackLimit
            expr: '(node_nf_conntrack_entries / node_nf_conntrack_entries_limit > 0.8)'
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: Host conntrack limit (instance {{ $$labels.instance }})
              description: "The number of conntrack is approaching limit\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostClockSkew
            expr: '((node_timex_offset_seconds > 0.05 and deriv(node_timex_offset_seconds[5m]) >= 0) or (node_timex_offset_seconds < -0.05 and deriv(node_timex_offset_seconds[5m]) <= 0))'
            for: 10m
            labels:
              severity: warning
            annotations:
              summary: Host clock skew (instance {{ $$labels.instance }})
              description: "Clock skew detected. Clock is out of sync. Ensure NTP is configured correctly on this host.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: HostClockNotSynchronising
            expr: '(min_over_time(node_timex_sync_status[1m]) == 0 and node_timex_maxerror_seconds >= 16)'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Host clock not synchronising (instance {{ $$labels.instance }})
              description: "Clock not synchronising. Ensure NTP is configured on this host.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

  vmalert_alerts_cadvisor_yml:
    content: |
      ---
      groups:
      - name: GoogleCadvisor
        rules:

          - alert: ContainerHighCpuUtilization
            expr: '(sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod, container) / sum(container_spec_cpu_quota{container!=""}/container_spec_cpu_period{container!=""}) by (pod, container) * 100) > 80'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Container High CPU utilization (instance {{ $$labels.instance }})
              description: "Container CPU utilization is above 80%\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: ContainerHighMemoryUsage
            expr: '(sum(container_memory_working_set_bytes{name!=""}) BY (instance, name) / sum(container_spec_memory_limit_bytes > 0) BY (instance, name) * 100) > 80'
            for: 2m
            labels:
              severity: warning
            annotations:
              summary: Container High Memory usage (instance {{ $$labels.instance }})
              description: "Container Memory usage is above 80%\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: ContainerHighThrottleRate
            expr: 'sum(increase(container_cpu_cfs_throttled_periods_total{container!=""}[5m])) by (container, pod, namespace) / sum(increase(container_cpu_cfs_periods_total[5m])) by (container, pod, namespace) > ( 25 / 100 )'
            for: 5m
            labels:
              severity: warning
            annotations:
              summary: Container high throttle rate (instance {{ $$labels.instance }})
              description: "Container is being throttled\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: ContainerHighLowChangeCpuUsage
            expr: '(abs((sum by (instance, name) (rate(container_cpu_usage_seconds_total{name!=""}[1m])) * 100) - (sum by (instance, name) (rate(container_cpu_usage_seconds_total{name!=""}[1m] offset 1m)) * 100)) or abs((sum by (instance, name) (rate(container_cpu_usage_seconds_total{name!=""}[1m])) * 100) - (sum by (instance, name) (rate(container_cpu_usage_seconds_total{name!=""}[5m] offset 1m)) * 100))) > 25'
            for: 0m
            labels:
              severity: info
            annotations:
              summary: Container high low change CPU usage (instance {{ $$labels.instance }})
              description: "This alert rule monitors the absolute change in CPU usage within a time window and triggers an alert when the change exceeds 25%.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: ContainerLowCpuUtilization
            expr: '(sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod, container) / sum(container_spec_cpu_quota{container!=""}/container_spec_cpu_period{container!=""}) by (pod, container) * 100) < 20'
            for: 7d
            labels:
              severity: info
            annotations:
              summary: Container Low CPU utilization (instance {{ $$labels.instance }})
              description: "Container CPU utilization is under 20% for 1 week. Consider reducing the allocated CPU.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

          - alert: ContainerLowMemoryUsage
            expr: '(sum(container_memory_working_set_bytes{name!=""}) BY (instance, name) / sum(container_spec_memory_limit_bytes > 0) BY (instance, name) * 100) < 20'
            for: 7d
            labels:
              severity: info
            annotations:
              summary: Container Low Memory usage (instance {{ $$labels.instance }})
              description: "Container Memory usage is under 20% for 1 week. Consider reducing the allocated memory.\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

  vmalert_alerts_caddy_yml:
    content: |
      ---
      groups:
      - name: Caddy
        rules:

        - alert: CaddyReverseProxyDown
          expr: count(caddy_reverse_proxy_upstreams_healthy) by (upstream) == 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Caddy Reverse Proxy Down (instance {{ $$labels.instance }})
            description: "All Caddy reverse proxies are down\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

        - alert: CaddyHighHttp4xxErrorRateService
          expr: sum(rate(caddy_http_request_duration_seconds_count{code=~"4.."}[3m])) by (instance) / sum(rate(caddy_http_request_duration_seconds_count[3m])) by (instance) * 100 > 5
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Caddy high HTTP 4xx error rate service (instance {{ $$labels.instance }})
            description: "Caddy service 4xx error rate is above 5%\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

        - alert: CaddyHighHttp5xxErrorRateService
          expr: sum(rate(caddy_http_request_duration_seconds_count{code=~"5.."}[3m])) by (instance) / sum(rate(caddy_http_request_duration_seconds_count[3m])) by (instance) * 100 > 5
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: Caddy high HTTP 5xx error rate service (instance {{ $$labels.instance }})
            description: "Caddy service 5xx error rate is above 5%\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

  vmalert_alerts_blackbox_yml:
    content: |
      ---
      groups:
      - name: blackbox
        rules:

        - alert: BlackboxProbeFailed
          expr: probe_success == 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Blackbox probe failed (instance {{ $$labels.instance }})
            description: "Probe failed\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

        - alert: BlackboxConfigurationReloadFailure
          expr: blackbox_exporter_config_last_reload_successful != 1
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Blackbox configuration reload failure (instance {{ $$labels.instance }})
            description: "Blackbox configuration reload failure\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

        - alert: BlackboxSlowProbe
          expr: avg_over_time(probe_duration_seconds[1m]) > 1
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox slow probe (instance {{ $$labels.instance }})
            description: "Blackbox probe took more than 1s to complete\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

        - alert: BlackboxProbeHttpFailure
          expr: probe_http_status_code <= 199 OR probe_http_status_code >= 400
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Blackbox probe HTTP failure (instance {{ $$labels.instance }})
            description: "HTTP status code is not 200-399\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

        - alert: BlackboxSslCertificateWillExpireSoon
          expr: 3 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 20
          for: 0m
          labels:
            severity: warning
          annotations:
            summary: Blackbox SSL certificate will expire soon (instance {{ $$labels.instance }})
            description: "SSL certificate expires in less than 20 days\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

        - alert: BlackboxSslCertificateWillExpireSoon
          expr: 0 <= round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 3
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Blackbox SSL certificate will expire soon (instance {{ $$labels.instance }})
            description: "SSL certificate expires in less than 3 days\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

        - alert: BlackboxSslCertificateExpired
          expr: round((last_over_time(probe_ssl_earliest_cert_expiry[10m]) - time()) / 86400, 0.1) < 0
          for: 0m
          labels:
            severity: critical
          annotations:
            summary: Blackbox SSL certificate expired (instance {{ $$labels.instance }})
            description: "SSL certificate has expired already\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

        - alert: BlackboxProbeSlowHttp
          expr: avg_over_time(probe_http_duration_seconds[1m]) > 1
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox probe slow HTTP (instance {{ $$labels.instance }})
            description: "HTTP request took more than 1s\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"

        - alert: BlackboxProbeSlowPing
          expr: avg_over_time(probe_icmp_duration_seconds[1m]) > 1
          for: 1m
          labels:
            severity: warning
          annotations:
            summary: Blackbox probe slow ping (instance {{ $$labels.instance }})
            description: "Blackbox ping took more than 1s\n  VALUE = {{ $$value }}\n  LABELS = {{ $$labels }}"
